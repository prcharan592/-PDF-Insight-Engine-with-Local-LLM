ğŸ“„ PDF Insight Engine with Local LLM

A Streamlit-based AI application that enables users to upload PDFs, process their content, and interact with them through conversational AI powered by locally running LLMs (Ollama). This project eliminates the need for an internet connection while ensuring efficient document analysis.

# ğŸš€ Features
âœ… Upload and process multiple PDFs
âœ… Conversational Q&A using local LLMs (Ollama)
âœ… Embeddings & Vector Search with FAISS and HuggingFace
âœ… Chunking & Retrieval using LangChain
âœ… Secure & Offline document analysis

# ğŸ› ï¸ Tech Stack
	â€¢	Python 3.9+
	â€¢	Streamlit (for the web interface)
	â€¢	LangChain (for document processing & retrieval)
	â€¢	FAISS (for efficient vector storage & retrieval)
	â€¢	HuggingFace Embeddings (sentence-transformers/all-MiniLM-L6-v2)
	â€¢	Ollama (for running local LLMs)

# Model	Used
Mistral 7B	4.1 GB	
Llama3 8B	4.7 GB	
Llama3.2 3B	2.0 GB
DeepSeek-R1 7B	4.7 GB

# ğŸ—ï¸ Installation & Setup

# 1ï¸âƒ£ Install Dependencies

# Clone the repository:
git clone https://github.com/yourusername/PDF-Insight-Engine.git
cd PDF-Insight-Engine

pip install -r requirements.txt


# 2ï¸âƒ£ Install & Run Ollama

Ollama is required to run the local LLM. Download and install it from:
ğŸ”— https://ollama.com

# Then, pull the required models:
ollama pull mistral
ollama pull llama3
ollama pull llama3:8b
ollama pull deepseek-r1


# â–¶ï¸ Running the Application

Start the Streamlit app:
streamlit run app.py


# ğŸ—ï¸ How It Works

1ï¸âƒ£ Upload PDFs
2ï¸âƒ£ Extract & Process Content using LangChain
3ï¸âƒ£ Convert text into embeddings (HuggingFace)
4ï¸âƒ£ Store & Retrieve vectors using FAISS
5ï¸âƒ£ Interact with PDFs through local LLM (Ollama)


# âš¡ Example Usage

Upload a PDF ğŸ“„ â†’ Ask â€œSummarize this document.â€ â†’ Get AI-powered response in seconds!
